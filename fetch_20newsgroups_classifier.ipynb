{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We download the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kJs7mubloThM"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train_unclear = fetch_20newsgroups(subset='train')\n",
    "newsgroups_train = fetch_20newsgroups (subset='train',remove =('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XYlAY8ebrofw"
   },
   "source": [
    "#### Check differences\n",
    "\n",
    "Since both datasets, with or without the meta-information, have the same length, we should continue with the second one, since it is more possible to avoid overfitting. This may happen because the removed information may contain words, or addresses that would cause large learning rates on traning dataset, but smaller ones on testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ub5eK6SvqiGG",
    "outputId": "6a842db9-1d6f-4677-aa76-83f40c7b4489"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same length\n"
     ]
    }
   ],
   "source": [
    "if (newsgroups_train_unclear.filenames.shape) == (newsgroups_train.filenames.shape):\n",
    "  print('Same length')\n",
    "else:\n",
    "  print('Different length')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mr5vWZ3yuc-L"
   },
   "source": [
    "#### Present the difference\n",
    "\n",
    "We print the first instance of these two training datasets to visualize their internal context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "id": "QgdvLZRXmnOn",
    "outputId": "e84d4d78-2b7b-4917-cd33-f466edc2c8eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With meta-information: \n",
      "\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Without meta-information: \n",
      "\n",
      "I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n"
     ]
    }
   ],
   "source": [
    "print(\"With meta-information: \\n\")\n",
    "print(newsgroups_train_unclear.data[0])\n",
    "\n",
    "print(\"Without meta-information: \\n\")\n",
    "print(newsgroups_train.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ylm3N5TSvDTX"
   },
   "source": [
    "#### Formulation of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "id": "CNHdjhrlusk2",
    "outputId": "7ca843a0-04a2-4d0c-f75b-796754772645"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training instances:  (11314,)\n",
      "Number of labels:  20\n",
      "Label names: \n",
      "\n",
      "sci.med\n",
      "talk.politics.misc\n",
      "comp.sys.mac.hardware\n",
      "comp.os.ms-windows.misc\n",
      "soc.religion.christian\n",
      "rec.motorcycles\n",
      "rec.sport.hockey\n",
      "talk.politics.mideast\n",
      "sci.crypt\n",
      "sci.electronics\n",
      "rec.autos\n",
      "misc.forsale\n",
      "talk.religion.misc\n",
      "sci.space\n",
      "comp.graphics\n",
      "talk.politics.guns\n",
      "comp.windows.x\n",
      "alt.atheism\n",
      "comp.sys.ibm.pc.hardware\n",
      "rec.sport.baseball\n",
      "Label ids inside the dataset: \n",
      "\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training instances: \", newsgroups_train.filenames.shape)\n",
    "print(\"Number of labels: \", len(newsgroups_train.target_names))\n",
    "\n",
    "print('Label names: \\n')\n",
    "for i in set(newsgroups_train.target_names):\n",
    "  print(i)\n",
    "\n",
    "print('Label ids inside the dataset: \\n')\n",
    "for i in set(newsgroups_train.target):\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wUhZn22Ox8S6"
   },
   "source": [
    "### Download some packages for text-processing.\n",
    "\n",
    "#### We moreover set some default stopwords for more experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "tytCM7ulx7Vu",
    "outputId": "0d9c3242-7225-40d3-a713-720404dce060"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Vaggelis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vaggelis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords_our = ['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at',\n",
    " 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \n",
    " 'can', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during',\n",
    " 'each', 'few', 'for', 'from', 'further', \n",
    " 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\",\n",
    " 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\",\n",
    " 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself',\n",
    " \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself',\n",
    " 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours' 'ourselves', 'out', 'over', 'own',\n",
    " 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', \n",
    " 'than', 'that',\"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \n",
    " \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', \n",
    " 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where',\n",
    " \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\",'will', 'with', \"won't\", 'would', \"wouldn't\", \n",
    " 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves', \n",
    " 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'hundred', 'thousand', '1st', '2nd', '3rd',\n",
    " '4th', '5th', '6th', '7th', '8th', '9th', '10th']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "feaiDl9LyOSV"
   },
   "source": [
    "## 2) Construct two Tokenizer functions \n",
    "\n",
    "We use the following text transformations :\n",
    "1.   Lower all characters\n",
    "2.   Tokenize each text segment, so as to obtain all the words, using the build in word_tokenize function\n",
    "3.   Remove some usual stopwords, for having a more clear version of each segment\n",
    "4.   Stem the remaining words, mapping some similar words as the same ones (e.g. faded or fading -> fade)\n",
    "5.   Remove words with length smaller than 3 characters\n",
    "\n",
    "--->>Two variants have been build here for being examined later!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "sZH4DxtwmPTI",
    "outputId": "83f199ee-ca23-435a-bcad-d30737edc90b"
   },
   "outputs": [],
   "source": [
    "def Tokenizer1(str_input, remove_stops = True, apply_stem = True, remove_short = True):\n",
    "    #lower all characters\n",
    "    str_input = str_input.lower()\n",
    "    \n",
    "    #separate all words in the input string\n",
    "    words = word_tokenize(str_input)\n",
    "\n",
    "    #we try to remove words with only 2 characters\n",
    "    if remove_short:\n",
    "      words = [word for word in words if len(word) > 2]\n",
    "    \n",
    "    #remove stopwords\n",
    "    if remove_stops:\n",
    "      stop_words = set(stopwords.words('english'))\n",
    "      words = [w for w in words if not w in stop_words]\n",
    "    \n",
    "    #stem the words\n",
    "    if apply_stem:\n",
    "      porter_stemmer=nltk.PorterStemmer()\n",
    "      words = [porter_stemmer.stem(word) for word in words]\n",
    "\n",
    "    return words\n",
    "\n",
    "def Tokenizer2(str_input, remove_stops = False, apply_stem = False, remove_short = False):\n",
    "    #lower all characters\n",
    "    str_input = str_input.lower()\n",
    "    \n",
    "    #separate all words in the input string\n",
    "    words = word_tokenize(str_input)\n",
    "    \n",
    "    #we try to remove words with only 2 characters\n",
    "    if remove_short:\n",
    "      words = [word for word in words if len(word) > 2]\n",
    "    \n",
    "    #remove stopwords\n",
    "    if remove_stops:\n",
    "      stop_words = set(stopwords.words('english'))\n",
    "      words = [w for w in words if not w in stop_words]\n",
    "    \n",
    "    #stem the words\n",
    "    if apply_stem:\n",
    "      porter_stemmer=nltk.PorterStemmer()\n",
    "      words = [porter_stemmer.stem(word) for word in words]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### examples of the above tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of using or not each component of pre=processing into the Tokenizer\n",
      "--------------------------------------------\n",
      "Tokenizer1 : lowers letters, tokenizes input strings, removes stop words, stems them and removes the short ones\n",
      "[\"n't\", 'sure', 'look', 'happen', 'japanes', 'citizen', 'world', 'war', \"'re\", 'prepar', 'say', 'let', 'round', 'peopl', 'stick', 'concentr', 'camp', 'without', 'trial', 'short', 'step', 'gass', 'without', 'trial', 'seem', 'nazi', 'origin', 'intend', 'imprison', 'jew', 'final', 'solut', 'dreamt', 'partli', 'could', \"n't\", 'afford', 'run', 'camp', 'devast', 'caus', 'goer', 'total', 'war', \"n't\", 'gass', 'gener', 'die', 'malnutrit', 'diseas']\n",
      "--------------------------------------------\n",
      "Tokenizer2 : only lowers letters and tokenizes the input string\n",
      "['do', \"n't\", 'be', 'so', 'sure', '.', 'look', 'what', 'happened', 'to', 'japanese', 'citizens', 'in', 'the', 'us', 'during', 'world', 'war', 'ii', '.', 'if', 'you', \"'re\", 'prepared', 'to', 'say', '``', 'let', \"'s\", 'round', 'these', 'people', 'up', 'and', 'stick', 'them', 'in', 'a', 'concentration', 'camp', 'without', 'trial', \"''\", ',', 'it', \"'s\", 'only', 'a', 'short', 'step', 'to', 'gassing', 'them', 'without', 'trial', '.', 'after', 'all', ',', 'it', 'seems', 'that', 'the', 'nazis', 'originally', 'only', 'intended', 'to', 'imprison', 'the', 'jews', ';', 'the', 'final', 'solution', 'was', 'dreamt', 'up', 'partly', 'because', 'they', 'could', \"n't\", 'afford', 'to', 'run', 'the', 'camps', 'because', 'of', 'the', 'devastation', 'caused', 'by', 'goering', \"'s\", 'total', 'war', '.', 'those', 'who', 'were', \"n't\", 'gassed', 'generally', 'died', 'of', 'malnutrition', 'or', 'disease', '.']\n"
     ]
    }
   ],
   "source": [
    "print('Tokenizer1 : lowers letters, tokenizes input strings, removes stop words, stems them and removes the short ones')\n",
    "print(Tokenizer1(newsgroups_train.data[15]))\n",
    "print('--------------------------------------------')\n",
    "print('Tokenizer2 : only lowers letters and tokenizes the input string')\n",
    "print(Tokenizer2(newsgroups_train.data[15]))\n",
    "\n",
    "#variations\n",
    "#print(Tokenizer(newsgroups_train.data[15], True,  False))\n",
    "#print(Tokenizer(newsgroups_train.data[15], False, True ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark : Looking at the printed results, we observe that indeed Tokenizer1 does not contain words like \"they\" (stop word) or \"it\" (short word). At the same time, the word \"happened\" of Tokenizer2 is stemmed into \"happen\" in Tokenizer1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "53pdcRlRGSc-"
   },
   "source": [
    "## 3) Defining the text classifiers\n",
    "\n",
    "We employ the tfidf transformation with the 2 variants of our build Tokenizers. Thus, we insert pipeline structures which contain the variants of the TfidfVectorizer along with 3 selected classifiers:\n",
    "\n",
    "1.   SGD classifier (ie linear classifier with SGD training )\n",
    "2.   Multinomial NB, a well-known classifier for textual data\n",
    "3.   GradientBoosting (GNB), a powerful predictive algorithm that is based on several weak estimators trying to catch the errors of previous iterations so as to learn the underlying problem.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "84m6CMyZ3IMT"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GB\n",
    "from sklearn.naive_bayes import MultinomialNB  as MNB\n",
    "from sklearn.linear_model import SGDClassifier as SGD\n",
    "\n",
    "\n",
    "text_clf1 = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(tokenizer=Tokenizer1, max_df=0.3, min_df=0.0002, max_features=1000)),\n",
    "    ('clf',   SGD(max_iter=1000)),\n",
    "])\n",
    "\n",
    "\n",
    "text_clf2 = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(tokenizer=Tokenizer2, max_df=0.3, min_df=0.0002, max_features=1000)),\n",
    "    ('clf',   SGD(max_iter=1000)),\n",
    "])\n",
    "\n",
    "text_clf3 = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(tokenizer=Tokenizer1, max_df=0.3, min_df=0.0002, max_features=1000)),\n",
    "    ('clf',   MNB()),\n",
    "])\n",
    "\n",
    "\n",
    "text_clf4 = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(tokenizer=Tokenizer2, max_df=0.3, min_df=0.0002, max_features=1000)),\n",
    "    ('clf',   MNB()),\n",
    "])\n",
    "\n",
    "text_clf5 = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(tokenizer=Tokenizer1, max_df=0.3, min_df=0.0002, max_features=1000)),\n",
    "    ('clf',   GB()),\n",
    "])\n",
    "\n",
    "\n",
    "text_clf6 = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(tokenizer=Tokenizer2, max_df=0.3, min_df=0.0002, max_features=1000)),\n",
    "    ('clf',   GB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Training the models ( over reduced version of the dataset ie consider 4 labels only - due to limited computational power - )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "HQ-qf1_JKw05",
    "outputId": "af4f558b-6ffc-4e42-ae81-708b69e41be1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train data:  (2034,)\n",
      "Size of test  data:  (1353,)\n"
     ]
    }
   ],
   "source": [
    "# Download the reduced training dataset along with its corresponding test set\n",
    "\n",
    "categories = ['alt.atheism', 'talk.religion.misc',\n",
    "              'comp.graphics', 'sci.space']\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups (subset='train',remove = ('headers', 'footers', 'quotes'), categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups (subset='test',  remove = ('headers', 'footers', 'quotes'), categories=categories)\n",
    "\n",
    "print('Size of train data: ', newsgroups_train.filenames.shape)\n",
    "print('Size of test  data: ', newsgroups_test.filenames.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "naTplv9b4SR4"
   },
   "outputs": [],
   "source": [
    "#Tokenizer1 + SGD clf\n",
    "text_clf1.fit(newsgroups_train.data, newsgroups_train.target)\n",
    "predictions_train1 = text_clf1.predict(newsgroups_train.data)\n",
    "predictions_test1 = text_clf1.predict(newsgroups_test.data)\n",
    "\n",
    "#Tokenizer2 + SGD clf\n",
    "text_clf2.fit(newsgroups_train.data, newsgroups_train.target)\n",
    "predictions_train2 = text_clf2.predict(newsgroups_train.data)\n",
    "predictions_test2 = text_clf2.predict(newsgroups_test.data)\n",
    "\n",
    "#Tokenizer1 + MNB clf\n",
    "text_clf3.fit(newsgroups_train.data, newsgroups_train.target)\n",
    "predictions_train3 = text_clf3.predict(newsgroups_train.data)\n",
    "predictions_test3 = text_clf3.predict(newsgroups_test.data)\n",
    "\n",
    "#Tokenizer2 + MNB clf\n",
    "text_clf4.fit(newsgroups_train.data, newsgroups_train.target)\n",
    "predictions_train4 = text_clf4.predict(newsgroups_train.data)\n",
    "predictions_test4 = text_clf4.predict(newsgroups_test.data)\n",
    "\n",
    "#Tokenizer1 + GB clf\n",
    "text_clf5.fit(newsgroups_train.data, newsgroups_train.target)\n",
    "predictions_train5 = text_clf5.predict(newsgroups_train.data)\n",
    "predictions_test5 = text_clf5.predict(newsgroups_test.data)\n",
    "\n",
    "#Tokenizer2 + GB clf\n",
    "text_clf6.fit(newsgroups_train.data, newsgroups_train.target)\n",
    "predictions_train6 = text_clf6.predict(newsgroups_train.data)\n",
    "predictions_test6 = text_clf6.predict(newsgroups_test.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Evaluating the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y4ZSnar4Lpkz"
   },
   "source": [
    "#### a) We can have a first sample (20 text data) of the predictions vs the reality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "qKb3eFH85H0k",
    "outputId": "bdc078bf-e6ee-474e-9933-88edcd6e42f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer1 + SGD clf, training set predictions :  [1 3 2 3 2 0 2 1 2 1 2 1 1 2 1 2 0 2 2 3]\n",
      "Tokenizer2 + SGD clf, training set predictions :  [1 3 2 3 2 0 2 1 2 1 2 1 2 2 1 2 0 2 2 3]\n",
      "                      training set real labels :  [1 3 2 0 2 0 2 1 2 1 2 1 1 2 1 2 0 2 2 3]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Tokenizer1 + SGD clf, test set predictions :  [2 1 1 1 1 1 2 2 0 0 1 1 1 2 1 0 3 1 1 2]\n",
      "Tokenizer2 + SGD clf, test set predictions :  [2 1 1 1 1 1 2 2 0 2 1 1 1 2 1 1 3 1 1 2]\n",
      "                      test set real labels :  [2 1 1 1 1 1 2 2 0 2 1 1 1 2 1 0 0 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "print('Tokenizer1 + SGD clf, training set predictions : ' ,predictions_train1[0:20])\n",
    "print('Tokenizer2 + SGD clf, training set predictions : ' ,predictions_train2[0:20])\n",
    "print('                      training set real labels : ', newsgroups_train.target[0:20])\n",
    "print('--------------------------------------------------------------------------------------------------')\n",
    "print('Tokenizer1 + SGD clf, test set predictions : ' ,predictions_test1[0:20])\n",
    "print('Tokenizer2 + SGD clf, test set predictions : ' ,predictions_test2[0:20])\n",
    "print('                      test set real labels : ' ,newsgroups_test.target[0:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jWlmtvXXQkl2"
   },
   "source": [
    "#### Or we could also obtain the final transformation of the input data based on their tfidf format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "13zPCqU86SJK",
    "outputId": "c4d3a816-5c13-4980-f65a-d623d60bfefa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 1000)\n",
      "(1353, 1000)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=Tokenizer1, max_df=0.3, min_df=0.0002, max_features=1000)\n",
    "\n",
    "X_tfidf_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "print(X_tfidf_train.shape)\n",
    "\n",
    "X_tfidf_test = vectorizer.fit_transform(newsgroups_test.data)\n",
    "print(X_tfidf_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T2RHAU2gMJWW"
   },
   "source": [
    "### b) Below obtain the overall behavior of the examined results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "3r8dhIk46g3-",
    "outputId": "720e6f55-7aa9-4f97-800e-71cd47cb5353",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer1 + SGD clf\n",
      "Accuracy: 0.6792313377679231\n",
      "Precision: 0.6749082463610018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.55      0.57       319\n",
      "           1       0.77      0.84      0.80       389\n",
      "           2       0.75      0.73      0.74       394\n",
      "           3       0.52      0.51      0.52       251\n",
      "\n",
      "    accuracy                           0.68      1353\n",
      "   macro avg       0.66      0.66      0.66      1353\n",
      "weighted avg       0.67      0.68      0.68      1353\n",
      "\n",
      "[[176  24  38  81]\n",
      " [ 20 327  31  11]\n",
      " [ 31  51 289  23]\n",
      " [ 72  25  27 127]]\n",
      "-------------------------------------------------------------------------------------------\n",
      "Tokenizer2 + SGD clf\n",
      "Accuracy: 0.656319290465632\n",
      "Precision: 0.6527905105178128\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.52      0.54       319\n",
      "           1       0.73      0.82      0.77       389\n",
      "           2       0.75      0.71      0.72       394\n",
      "           3       0.51      0.50      0.51       251\n",
      "\n",
      "    accuracy                           0.66      1353\n",
      "   macro avg       0.64      0.64      0.64      1353\n",
      "weighted avg       0.65      0.66      0.65      1353\n",
      "\n",
      "[[165  31  43  80]\n",
      " [ 18 319  36  16]\n",
      " [ 37  56 278  23]\n",
      " [ 76  33  16 126]]\n",
      "-------------------------------------------------------------------------------------------\n",
      "Tokenizer1 + MNB clf\n",
      "Accuracy: 0.7280118255728012\n",
      "Precision: 0.7253874404658738\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.68      0.64       319\n",
      "           1       0.83      0.87      0.85       389\n",
      "           2       0.75      0.85      0.79       394\n",
      "           3       0.67      0.39      0.49       251\n",
      "\n",
      "    accuracy                           0.73      1353\n",
      "   macro avg       0.72      0.70      0.69      1353\n",
      "weighted avg       0.73      0.73      0.72      1353\n",
      "\n",
      "[[218  16  40  45]\n",
      " [ 13 337  37   2]\n",
      " [ 24  37 333   0]\n",
      " [104  14  36  97]]\n",
      "-------------------------------------------------------------------------------------------\n",
      "Tokenizer2 + MNB clf\n",
      "Accuracy: 0.6991869918699187\n",
      "Precision: 0.7018857809598974\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.66      0.61       319\n",
      "           1       0.81      0.84      0.82       389\n",
      "           2       0.72      0.83      0.77       394\n",
      "           3       0.69      0.33      0.44       251\n",
      "\n",
      "    accuracy                           0.70      1353\n",
      "   macro avg       0.69      0.66      0.66      1353\n",
      "weighted avg       0.70      0.70      0.69      1353\n",
      "\n",
      "[[211  23  52  33]\n",
      " [ 17 326  44   2]\n",
      " [ 33  32 327   2]\n",
      " [115  23  31  82]]\n",
      "-------------------------------------------------------------------------------------------\n",
      "Tokenizer1 + GB clf\n",
      "Accuracy: 0.70509977827051\n",
      "Precision: 0.7014478897766909\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.61      0.62       319\n",
      "           1       0.83      0.82      0.82       389\n",
      "           2       0.69      0.82      0.75       394\n",
      "           3       0.61      0.47      0.53       251\n",
      "\n",
      "    accuracy                           0.71      1353\n",
      "   macro avg       0.69      0.68      0.68      1353\n",
      "weighted avg       0.70      0.71      0.70      1353\n",
      "\n",
      "[[193  14  55  57]\n",
      " [ 14 319  49   7]\n",
      " [ 25  34 325  10]\n",
      " [ 75  18  41 117]]\n",
      "-------------------------------------------------------------------------------------------\n",
      "Tokenizer2 + GB clf\n",
      "Accuracy: 0.6851441241685144\n",
      "Precision: 0.6849141550374078\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.57      0.56       319\n",
      "           1       0.85      0.80      0.83       389\n",
      "           2       0.69      0.80      0.74       394\n",
      "           3       0.59      0.46      0.52       251\n",
      "\n",
      "    accuracy                           0.69      1353\n",
      "   macro avg       0.67      0.66      0.66      1353\n",
      "weighted avg       0.68      0.69      0.68      1353\n",
      "\n",
      "[[182  17  53  67]\n",
      " [ 23 313  49   4]\n",
      " [ 42  26 316  10]\n",
      " [ 82  11  42 116]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, classification_report, confusion_matrix\n",
    "\n",
    "print('Tokenizer1 + SGD clf')\n",
    "print(\"Accuracy:\", accuracy_score(newsgroups_test.target, predictions_test1))\n",
    "print(\"Precision:\", precision_score(newsgroups_test.target, predictions_test1, average='weighted'))\n",
    "print(classification_report(newsgroups_test.target, predictions_test1))\n",
    "print(confusion_matrix(newsgroups_test.target, predictions_test1))\n",
    "print('-------------------------------------------------------------------------------------------')\n",
    "print('Tokenizer2 + SGD clf')\n",
    "print(\"Accuracy:\", accuracy_score(newsgroups_test.target, predictions_test2))\n",
    "print(\"Precision:\", precision_score(newsgroups_test.target, predictions_test2, average='weighted'))\n",
    "print(classification_report(newsgroups_test.target, predictions_test2))\n",
    "print(confusion_matrix(newsgroups_test.target, predictions_test2))\n",
    "print('-------------------------------------------------------------------------------------------')\n",
    "print('Tokenizer1 + MNB clf')\n",
    "print(\"Accuracy:\", accuracy_score(newsgroups_test.target, predictions_test3))\n",
    "print(\"Precision:\", precision_score(newsgroups_test.target, predictions_test3, average='weighted'))\n",
    "print(classification_report(newsgroups_test.target, predictions_test3))\n",
    "print(confusion_matrix(newsgroups_test.target, predictions_test3))\n",
    "print('-------------------------------------------------------------------------------------------')\n",
    "print('Tokenizer2 + MNB clf')\n",
    "print(\"Accuracy:\", accuracy_score(newsgroups_test.target, predictions_test4))\n",
    "print(\"Precision:\", precision_score(newsgroups_test.target, predictions_test4, average='weighted'))\n",
    "print(classification_report(newsgroups_test.target, predictions_test4))\n",
    "print(confusion_matrix(newsgroups_test.target, predictions_test4))\n",
    "print('-------------------------------------------------------------------------------------------')\n",
    "print('Tokenizer1 + GB clf')\n",
    "print(\"Accuracy:\", accuracy_score(newsgroups_test.target, predictions_test5))\n",
    "print(\"Precision:\", precision_score(newsgroups_test.target, predictions_test5, average='weighted'))\n",
    "print(classification_report(newsgroups_test.target, predictions_test5))\n",
    "print(confusion_matrix(newsgroups_test.target, predictions_test5))\n",
    "print('-------------------------------------------------------------------------------------------')\n",
    "print('Tokenizer2 + GB clf')\n",
    "print(\"Accuracy:\", accuracy_score(newsgroups_test.target, predictions_test6))\n",
    "print(\"Precision:\", precision_score(newsgroups_test.target, predictions_test6, average='weighted'))\n",
    "print(classification_report(newsgroups_test.target, predictions_test6))\n",
    "print(confusion_matrix(newsgroups_test.target, predictions_test6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary :**\n",
    "\n",
    "**1) overall we see that in ALL cases we see that the FIRST variant ( ie remove stopwords and small words and include stemming ) of the Tokenizer is the best.**\n",
    "\n",
    "**2) overall among all six classifiers the one that performs better is the MultinomialNaiveBayes classifier via the first variant of the Tokenizer function.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v2HgQNoxqHW4"
   },
   "source": [
    "## 6) Further analysis of the best classifier (Tokenizer1 + MNB) using n-grams\n",
    "\n",
    "So far, in all aforementioned classifiers, the ngram_range parameter was set to the default value of (1,1), meaning that the transformer considered only unigrams of the input text. In other words, so far every single word was independent of its previous word.\n",
    "\n",
    "However, it seems very logical to wonder if we could get better classifying results, in case we also allow bigrams ( ie each two succesive words are dependent of each other ) in the training.\n",
    "\n",
    "#### Below we examine 3 variants of the above classifier:\n",
    "\n",
    "1.   Default arguments of TfidfVectorizer, ie ngram_range = (1,1) \n",
    "2.   Use of unigrams and bigrams, ie. ngram_range = (1,2) \n",
    "3.   Use of bigrams only, ie. ngram_range = (2,2) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  \"Define - Train - Test\" each variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nztzr1HVvosf"
   },
   "outputs": [],
   "source": [
    "# Only unigrams\n",
    "text_clf3_variant1 = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(tokenizer=Tokenizer1)),\n",
    "    ('clf',   MNB()),])\n",
    "text_clf3_variant1.fit(newsgroups_train.data, newsgroups_train.target)\n",
    "predictions_test1 = text_clf3_variant1.predict(newsgroups_test.data)\n",
    "\n",
    "# Unigrams + Bigrams\n",
    "text_clf3_variant2 = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(tokenizer=Tokenizer1, ngram_range = (1,2))),\n",
    "    ('clf',   MNB()),])\n",
    "text_clf3_variant2.fit(newsgroups_train.data, newsgroups_train.target)\n",
    "predictions_test2 = text_clf3_variant2.predict(newsgroups_test.data)\n",
    "\n",
    "# Only bigrams\n",
    "text_clf3_variant3 = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(tokenizer=Tokenizer1, ngram_range = (2,2))),\n",
    "    ('clf',   MNB()),])\n",
    "text_clf3_variant3.fit(newsgroups_train.data, newsgroups_train.target)\n",
    "predictions_test3 = text_clf3_variant3.predict(newsgroups_test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O1SACWCXqqxW"
   },
   "source": [
    "#### Evaluation of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 884
    },
    "colab_type": "code",
    "id": "W8N4Ti1nqsTQ",
    "outputId": "2a9b9e9a-fe24-44de-ae9f-86226baa7cde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only unigrams\n",
      "Accuracy: 0.7287509238728751\n",
      "Precision: 0.74671267456911\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.71      0.65       319\n",
      "           1       0.88      0.89      0.89       389\n",
      "           2       0.71      0.91      0.80       394\n",
      "           3       0.80      0.21      0.33       251\n",
      "\n",
      "    accuracy                           0.73      1353\n",
      "   macro avg       0.74      0.68      0.67      1353\n",
      "weighted avg       0.75      0.73      0.70      1353\n",
      "\n",
      "[[227  14  65  13]\n",
      " [  4 347  38   0]\n",
      " [ 14  21 359   0]\n",
      " [139  13  46  53]]\n",
      "---------------------------------------------------------------------------------------------\n",
      "Unigrams + Bigrams\n",
      "Accuracy: 0.7184035476718403\n",
      "Precision: 0.7473780990379352\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.71      0.65       319\n",
      "           1       0.89      0.87      0.88       389\n",
      "           2       0.66      0.93      0.77       394\n",
      "           3       0.84      0.17      0.28       251\n",
      "\n",
      "    accuracy                           0.72      1353\n",
      "   macro avg       0.75      0.67      0.65      1353\n",
      "weighted avg       0.75      0.72      0.68      1353\n",
      "\n",
      "[[225  15  71   8]\n",
      " [  3 340  46   0]\n",
      " [  9  20 365   0]\n",
      " [133   9  67  42]]\n",
      "---------------------------------------------------------------------------------------------\n",
      "Only bigrams\n",
      "Accuracy: 0.6097560975609756\n",
      "Precision: 0.6413831649670277\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.45      0.52       319\n",
      "           1       0.68      0.83      0.75       389\n",
      "           2       0.54      0.84      0.66       394\n",
      "           3       0.76      0.10      0.18       251\n",
      "\n",
      "    accuracy                           0.61      1353\n",
      "   macro avg       0.65      0.56      0.53      1353\n",
      "weighted avg       0.64      0.61      0.56      1353\n",
      "\n",
      "[[144  49 118   8]\n",
      " [  5 323  61   0]\n",
      " [  8  54 332   0]\n",
      " [ 75  51  99  26]]\n"
     ]
    }
   ],
   "source": [
    "print('Only unigrams')\n",
    "print(\"Accuracy:\", accuracy_score(newsgroups_test.target, predictions_test1))\n",
    "print(\"Precision:\", precision_score(newsgroups_test.target, predictions_test1, average='weighted'))\n",
    "print(classification_report(newsgroups_test.target, predictions_test1))\n",
    "print(confusion_matrix(newsgroups_test.target, predictions_test1))\n",
    "print('---------------------------------------------------------------------------------------------')\n",
    "print('Unigrams + Bigrams')\n",
    "print(\"Accuracy:\", accuracy_score(newsgroups_test.target, predictions_test2))\n",
    "print(\"Precision:\", precision_score(newsgroups_test.target, predictions_test2, average='weighted'))\n",
    "print(classification_report(newsgroups_test.target, predictions_test2))\n",
    "print(confusion_matrix(newsgroups_test.target, predictions_test2))\n",
    "print('---------------------------------------------------------------------------------------------')\n",
    "print('Only bigrams')\n",
    "print(\"Accuracy:\", accuracy_score(newsgroups_test.target, predictions_test3))\n",
    "print(\"Precision:\", precision_score(newsgroups_test.target, predictions_test3, average='weighted'))\n",
    "print(classification_report(newsgroups_test.target, predictions_test3))\n",
    "print(confusion_matrix(newsgroups_test.target, predictions_test3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: We observe the better generalization ability over test data when we use both unigrams and bigrams!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6hSay3awsEon"
   },
   "source": [
    "## 7) Tuning and Evaluating the final model\n",
    "\n",
    "#### We consider the best classifier found so far ( Tokenizer1+MNB+Unigrams+Bigrams) and we use a grid search in order to find better parameters for the whole pipeline. \n",
    "\n",
    "#### More experiments can be selected, here we have created a grid of 24 separate cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "0Hk2FpgPer07",
    "outputId": "a0548c5e-6ced-47b8-897e-2b97277f8e9b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:823: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
      "  \"removed in 0.24.\", FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.792\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.9\n",
      "\ttfidf__max_df: 0.75\n",
      "\ttfidf__max_features: 5000\n",
      "\ttfidf__norm: 'l2'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    'tfidf__max_features': (5000, None),\n",
    "    'tfidf__max_df': (1, 0.75, 0.5),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': (0.9, 1),\n",
    "    #'clf__n_estimators': (50, 100, 250),\n",
    "}\n",
    "\n",
    "text_tune = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(tokenizer=Tokenizer1, ngram_range = (1,2))),\n",
    "    ('clf',   MNB()),\n",
    "])\n",
    "\n",
    "\n",
    "gs_clf = GridSearchCV(text_tune, parameters, cv=3, iid=False, n_jobs=-1)\n",
    "gs_clf.fit(newsgroups_train.data, newsgroups_train.target)\n",
    "\n",
    "print(\"Best score: %0.3f\" % gs_clf.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = gs_clf.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oYVbSIou3CuY"
   },
   "source": [
    "### Below we apply the best parameters and indeed obtain better predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "OsRXYk_fXlju",
    "outputId": "a55b48da-a05a-4e07-e0f6-a5abd301e97b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7590539541759054\n",
      "Precision: 0.7646233573261035\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.72      0.67       319\n",
      "           1       0.88      0.90      0.89       389\n",
      "           2       0.77      0.88      0.82       394\n",
      "           3       0.77      0.39      0.52       251\n",
      "\n",
      "    accuracy                           0.76      1353\n",
      "   macro avg       0.76      0.72      0.72      1353\n",
      "weighted avg       0.76      0.76      0.75      1353\n",
      "\n",
      "[[231  13  47  28]\n",
      " [  8 351  29   1]\n",
      " [ 22  25 347   0]\n",
      " [113  11  29  98]]\n"
     ]
    }
   ],
   "source": [
    "text_clf_tuned = text_tune.set_params(**best_parameters)\n",
    "text_clf_tuned.fit(newsgroups_train.data, newsgroups_train.target)\n",
    "predictions_train = text_clf_tuned.predict(newsgroups_train.data)\n",
    "predictions_test = text_clf_tuned.predict(newsgroups_test.data)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(newsgroups_test.target, predictions_test))\n",
    "print(\"Precision:\", precision_score(newsgroups_test.target, predictions_test, average='weighted'))\n",
    "print(classification_report(newsgroups_test.target, predictions_test))\n",
    "print(confusion_matrix(newsgroups_test.target, predictions_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Behaviour of the final model over the 20-label dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hzFtkvvqzxJc",
    "outputId": "c401b021-6a76-4ae8-d157-52c18546de0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6477695167286245\n",
      "Precision: 0.6699598130689187\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.32      0.40       319\n",
      "           1       0.54      0.64      0.58       389\n",
      "           2       0.61      0.55      0.58       394\n",
      "           3       0.58      0.63      0.60       392\n",
      "           4       0.66      0.58      0.62       385\n",
      "           5       0.71      0.73      0.72       395\n",
      "           6       0.78      0.74      0.76       390\n",
      "           7       0.70      0.67      0.68       396\n",
      "           8       0.75      0.70      0.73       398\n",
      "           9       0.86      0.77      0.81       397\n",
      "          10       0.55      0.91      0.69       399\n",
      "          11       0.72      0.72      0.72       396\n",
      "          12       0.61      0.50      0.55       393\n",
      "          13       0.78      0.69      0.73       396\n",
      "          14       0.78      0.72      0.75       394\n",
      "          15       0.45      0.88      0.59       398\n",
      "          16       0.56      0.73      0.63       364\n",
      "          17       0.79      0.75      0.77       376\n",
      "          18       0.67      0.31      0.42       310\n",
      "          19       0.79      0.06      0.11       251\n",
      "\n",
      "    accuracy                           0.65      7532\n",
      "   macro avg       0.67      0.63      0.62      7532\n",
      "weighted avg       0.67      0.65      0.64      7532\n",
      "\n",
      "[[101   4   1   0   0   3   0   4   6   5  10   5   1   8   8 132  13  14\n",
      "    2   2]\n",
      " [  3 248  22  13  12  37   5   2   5   1   7  10   6   2   9   5   1   1\n",
      "    0   0]\n",
      " [  2  31 217  41  15  33   4   2   5   2  18   6   2   5   5   4   0   1\n",
      "    1   0]\n",
      " [  0   9  37 248  33   9  10   5   0   0   7   7  24   0   1   0   2   0\n",
      "    0   0]\n",
      " [  0  13  25  47 223   6  18   5   0   1  15   7  16   2   3   3   1   0\n",
      "    0   0]\n",
      " [  0  45  13   5   8 289   6   4   0   2   8   6   2   0   2   4   1   0\n",
      "    0   0]\n",
      " [  0   2   3  32  14   1 288  12   6   4  11   2   6   2   3   2   0   1\n",
      "    1   0]\n",
      " [  2   3   3   2   2   4   9 266  24   5  31   3  16   4   7   4   4   4\n",
      "    3   0]\n",
      " [  0   3   0   1   3   1   5  30 280   9  21   3  12   5   1  13   4   3\n",
      "    4   0]\n",
      " [  2   8   1   1   2   5   2   3   3 307  35   2   1   6   0  13   3   2\n",
      "    1   0]\n",
      " [  1   2   0   1   0   1   0   2   1   5 365   5   2   1   1   8   3   0\n",
      "    1   0]\n",
      " [  1  10   8   3   5   5   4   4   1   1  20 285   4   3   8   7  19   6\n",
      "    2   0]\n",
      " [  2  30  15  31  17   4  11  12  13   1  12  23 196   8   8   4   3   3\n",
      "    0   0]\n",
      " [  4  17   2   1   2   3   5  11   8   4  20   1  10 272   5  17   7   4\n",
      "    3   0]\n",
      " [  2  14   4   0   0   2   0   8   5   1  21   5  14   9 283  15   4   2\n",
      "    5   0]\n",
      " [  8   8   1   0   0   1   2   0   0   1  14   2   1   4   2 350   2   1\n",
      "    1   0]\n",
      " [  5   2   2   0   0   1   1   4   5   0  17  11   2   7   3  20 267   9\n",
      "    7   1]\n",
      " [ 14   5   1   1   0   1   0   1   6   3   7   3   2   2   0  29   6 283\n",
      "   12   0]\n",
      " [ 13   1   0   0   1   2   0   3   5   2  11   8   4   4  11  28 104  16\n",
      "   96   1]\n",
      " [ 27   4   2   1   0   1   1   3   1   4  11   2   1   4   5 125  33   7\n",
      "    4  15]]\n"
     ]
    }
   ],
   "source": [
    "newsgroups_train = fetch_20newsgroups (subset='train',remove = ('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups (subset='test',  remove = ('headers', 'footers', 'quotes'))\n",
    "\n",
    "\n",
    "text_clf_tuned.fit(newsgroups_train.data, newsgroups_train.target)\n",
    "predictions_train2 = text_clf_tuned.predict(newsgroups_train.data)\n",
    "predictions_test2 = text_clf_tuned.predict(newsgroups_test.data)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(newsgroups_test.target, predictions_test2))\n",
    "print(\"Precision:\", precision_score(newsgroups_test.target, predictions_test2, average='weighted'))\n",
    "print(classification_report(newsgroups_test.target, predictions_test2))\n",
    "print(confusion_matrix(newsgroups_test.target, predictions_test2))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
